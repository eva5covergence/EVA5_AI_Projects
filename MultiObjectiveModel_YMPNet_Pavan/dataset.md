# Dataset

**Data Collection:**

We are solving 3 different problems (Bounding box prediciton, Depthmap, and Planer surfaces detections) through single multi objective model. For object detection we collected images of workers who are wearing vest, hard-hat, mask and boots (3500 images approximately). But this data is not enough to train for depthmap and planer surfaces prediction. So we collected even more data of house interior images from youtube videos. Here the challenge is collecting images should have only interior objects without human beings.

So we filtered the collected images using maskRCNN object predition which trained on coco dataset of 80 classes. If it detects any person object in the image, it will delete that image from collected images.

**Following steps taken to filter out the images:**

- Prepare 5000+ images for Midas and PlanerRCNN:
    - Automated below steps
        - Collected Interior locations related videos youtube links (manual)
        - Installed youtube-dl
        - Installed Detectron2 and dependencies 
        - Downloaded all youtube videos through youtube-dl
        - Extracted the frames for every second
        - Deleted the frames/images which are “having person” or “no objects yet all” by detecting through detectron2 using maskRCNN model
        - Copied all filtered images to target location. 

The dataset used for this model is called PPE dataset. This dataset consists of people wearing protective gear like Hardhat, Vest, Mask and Boots. About 3570 number of images of this kind was randomly picked from the internet and was used as input data. For each input image, we have corresponding Bounding box coordinates for 4 classes, Depth image, Surface Plane image, Plane parameters and Plane mask as the ground truth.
 - The bounding box coordinates was created using Yolo annotation tool and the results were saved in a text file.
 - The Depth image was generated from the [MIDAS](https://github.com/intel-isl/MiDaS) network by running the model in evaluation mode and infering the depth output for given image.
 - The Surface Plane image, Plane parameters and Plane mask were again generated by running the pretrained [PlaneRCNN model](https://github.com/NVlabs/planercnn) in evaluation mode
Go to this link to view the [Input Images and Ground Truth](https://drive.google.com/drive/folders/1_ZW5IfMjX5bmNSpTmZKpvgqk5hybBttl?usp=sharing)

## Preview

### Input Image

![](Images/InputImages.jpg)

### Depth Image

![](Images/Depth_Inputs.jpg)

### Plane Segmentation Image

![](Images/PlaneRCNN_Inputs.jpg)

## Data Augumentation
- The input images are resized to 448 x448, Letter box augumentation is applied.
- The Bounding Box values are normalized and resized according the resized input image.
- No augmentation are applied to the ground truth images as it would distort them from their corresponding labels.

## Data Loading
Data preparation script is created to list down all the images in the input data folder. The list is split into 80:20 train and test set. This train and test set is written into a text file named [train.txt](https://github.com/eva5covergence/EVA5_AI_Projects_new/blob/master/MultiObjectiveModel_YMPNet_Pavan/data/customdata/train.txt) and [test.txt](https://github.com/eva5covergence/EVA5_AI_Projects_new/blob/master/MultiObjectiveModel_YMPNet_Pavan/data/customdata/test.txt). The input images are read and the size of the images are written into [train.shape](https://github.com/eva5covergence/EVA5_AI_Projects_new/blob/master/MultiObjectiveModel_YMPNet_Pavan/data/customdata/train.shapes) and [test.shape](https://github.com/eva5covergence/EVA5_AI_Projects_new/blob/master/MultiObjectiveModel_YMPNet_Pavan/data/customdata/test.shapes)
The corresponding ground truth depth image, segmentation image, plane_mask and plane_parameter is written into a csv file. The Dataloader class for this project takes the train.txt as input parameter and extracts the input image name to load the image, load the corresponding depth image, segmentation image, plane_mask and plane_parameter. The dataloader class applies the basic image augmentation for the input image, resize and normalize the bounding box values. No augmentation are applied for depth and segmentation images. The dataloader class returns following parameter
![](Images/Dataloader.jpg)

The train script calls the dataloader class to get the data for train and test. 
